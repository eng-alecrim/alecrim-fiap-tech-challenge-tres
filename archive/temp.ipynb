{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from src.utils import get_path_projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/6w_d4rjs17n_cdn1lnl_nnf00000gn/T/ipykernel_35367/4034210821.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train RMSE 2674.3618, time 258.6779 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 90\u001b[0m\n\u001b[1;32m     86\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(X, y), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10_000\u001b[39m)\n\u001b[1;32m     88\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2_000\u001b[39m\n\u001b[0;32m---> 90\u001b[0m rede_neural \u001b[38;5;241m=\u001b[39m \u001b[43mrotina_treino\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodelo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrede_neural\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_perda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43motimizador\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motimizador\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 66\u001b[0m, in \u001b[0;36mrotina_treino\u001b[0;34m(modelo, fn_perda, otimizador, data_loader, num_epochs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\n\u001b[1;32m     65\u001b[0m     otimizador\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     otimizador\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/workspace/alecrim-fiap-tech-challenge-tres/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/alecrim-fiap-tech-challenge-tres/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/alecrim-fiap-tech-challenge-tres/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. Carregando os dados\n",
    "dir_projeto = get_path_projeto()\n",
    "path_csv = dir_projeto / \"data/staged/dados_empilhados.csv\"\n",
    "config_csv = {\"sep\": \"\\t\", \"encoding\": \"utf-8\"}\n",
    "\n",
    "dataset = pd.read_csv(path_csv, **config_csv)\n",
    "\n",
    "# 2. Selecionando apenas dados sobre a geração de energia eólica\n",
    "wind_power_generation = dataset.loc[:, [\"interval_start_local\", \"wind\"]]\n",
    "wind_power_generation.rename(\n",
    "    columns={\"interval_start_local\": \"date\", \"wind\": \"power_generation\"}, inplace=True\n",
    ")\n",
    "\n",
    "# 3. Dados para treinar o modelo\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "def create_dataset(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "\n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - lookback):\n",
    "        feature = dataset[i : i + lookback]\n",
    "        target = dataset[i + 1 : i + lookback + 1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "lookback = 1\n",
    "X, y = create_dataset(\n",
    "    dataset=wind_power_generation[\"power_generation\"].values, lookback=lookback\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Criando a rede neural\n",
    "class ArquiteturaRedeNeural(nn.Module):\n",
    "    def __init__(self, lookback) -> None:\n",
    "        super(ArquiteturaRedeNeural, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lookback, hidden_size=50, num_layers=2, batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=50, out_features=lookback)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 5. Treinando o modelo\n",
    "def rotina_treino(modelo, fn_perda, otimizador, data_loader, num_epochs):\n",
    "    t0 = time()\n",
    "    for epoch in range(num_epochs):\n",
    "        modelo.train()\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            y_pred = modelo(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            otimizador.zero_grad()\n",
    "            loss.backward()\n",
    "            otimizador.step()\n",
    "        if (epoch + 1) % 10 != 0:\n",
    "            continue\n",
    "        modelo.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = modelo(X)\n",
    "            train_rmse = np.sqrt(loss_fn(y_pred, y))\n",
    "        exec_time = time() - t0\n",
    "        print(\n",
    "            \"Epoch %d: train RMSE %.4f, time %.4f s\"\n",
    "            % (epoch + 1, train_rmse, exec_time)\n",
    "        )\n",
    "        t0 = time()\n",
    "    return modelo\n",
    "\n",
    "\n",
    "rede_neural = ArquiteturaRedeNeural(lookback=lookback)\n",
    "loss_fn = nn.MSELoss()\n",
    "otimizador = Adam(params=rede_neural.parameters(), lr=0.05)\n",
    "data_loader = DataLoader(TensorDataset(X, y), shuffle=True, batch_size=10_000)\n",
    "\n",
    "num_epochs = 2_000\n",
    "\n",
    "rede_neural = rotina_treino(\n",
    "    modelo=rede_neural,\n",
    "    fn_perda=loss_fn,\n",
    "    otimizador=otimizador,\n",
    "    data_loader=data_loader,\n",
    "    num_epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUGESTÃO GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from src.utils import get_path_projeto\n",
    "\n",
    "dir_projeto = get_path_projeto()\n",
    "\n",
    "# 1. Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Load data and prepare it as usual\n",
    "# (Assume dataset loading code here)\n",
    "\n",
    "\n",
    "# 3. Modify the create_dataset function to use the device\n",
    "def create_dataset(dataset, lookback):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - lookback):\n",
    "        feature = dataset[i : i + lookback]\n",
    "        target = dataset[i + 1 : i + lookback + 1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return (\n",
    "        torch.tensor(X, dtype=torch.float32).to(device),\n",
    "        torch.tensor(y, dtype=torch.float32).to(device),\n",
    "    )\n",
    "\n",
    "\n",
    "lookback = 1\n",
    "X, y = create_dataset(\n",
    "    dataset=wind_power_generation[\"power_generation\"].values, lookback=lookback\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Update the model to use the device\n",
    "class ArquiteturaRedeNeural(nn.Module):\n",
    "    def __init__(self, lookback) -> None:\n",
    "        super(ArquiteturaRedeNeural, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lookback, hidden_size=50, num_layers=2, batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=50, out_features=lookback)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Move model to the device\n",
    "rede_neural = ArquiteturaRedeNeural(lookback=lookback).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "otimizador = Adam(params=rede_neural.parameters(), lr=0.05)\n",
    "\n",
    "# 5. Update the data loader\n",
    "data_loader = DataLoader(TensorDataset(X, y), shuffle=True, batch_size=10_000)\n",
    "\n",
    "\n",
    "# 6. Train the model with the routine updated for the device\n",
    "def rotina_treino(modelo, fn_perda, otimizador, data_loader, num_epochs):\n",
    "    t0 = time()\n",
    "    for epoch in range(num_epochs):\n",
    "        modelo.train()\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            # Move batches to device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = modelo(X_batch)\n",
    "            loss = fn_perda(y_pred, y_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            otimizador.zero_grad()\n",
    "            loss.backward()\n",
    "            otimizador.step()\n",
    "\n",
    "        # Evaluation step every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            modelo.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = modelo(X)\n",
    "                train_rmse = np.sqrt(fn_perda(y_pred, y).item())\n",
    "            exec_time = time() - t0\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}: train RMSE {train_rmse:.4f}, time {exec_time:.4f} s\"\n",
    "            )\n",
    "            t0 = time()\n",
    "\n",
    "    return modelo\n",
    "\n",
    "\n",
    "# Start training\n",
    "num_epochs = 2000\n",
    "rede_neural = rotina_treino(\n",
    "    modelo=rede_neural,\n",
    "    fn_perda=loss_fn,\n",
    "    otimizador=otimizador,\n",
    "    data_loader=data_loader,\n",
    "    num_epochs=num_epochs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
